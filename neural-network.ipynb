{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural Network","metadata":{}},{"cell_type":"markdown","source":"## One Liner Definition\nNeural Network is a mathematical function. \n\nThis mathematical function is based on inputs and parameters.\n\nInputs and parameters are multiplied and added them up. Negative values are set to zero.\n\nThese operations are repeated untill the error of prediction is minimized.","metadata":{}},{"cell_type":"markdown","source":"## Motivation\n\nThose 3 simple steps are the foundation of any deep learning model.\n\nImplicitally they touch most important parts of NN:\n1. Inputs and parameters are multiplied and added them up => [Matrix multiplication]();\n2. Negative values are set to zero => [Rectified Linear function]();\n3. Operations repeated untill error of prediction is minimized => [Gradient descent]() on [Loss function]().\n\nThe most complex deep learning model is built on these foundamentals. Deeply understanding them will help to breaking every complex model out.","metadata":{}},{"cell_type":"markdown","source":"## Implementation","metadata":{}},{"cell_type":"code","source":"def f(x): return  3*x**2 + 2*x + 1","metadata":{"execution":{"iopub.status.busy":"2022-12-07T14:19:01.486613Z","iopub.execute_input":"2022-12-07T14:19:01.487193Z","iopub.status.idle":"2022-12-07T14:19:01.517196Z","shell.execute_reply.started":"2022-12-07T14:19:01.487087Z","shell.execute_reply":"2022-12-07T14:19:01.516187Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def quad(a, b, c, x): return a*x**2 + b*x + c","metadata":{"execution":{"iopub.status.busy":"2022-12-07T14:19:08.835728Z","iopub.execute_input":"2022-12-07T14:19:08.836517Z","iopub.status.idle":"2022-12-07T14:19:08.842154Z","shell.execute_reply.started":"2022-12-07T14:19:08.836473Z","shell.execute_reply":"2022-12-07T14:19:08.840990Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T14:19:16.345569Z","iopub.execute_input":"2022-12-07T14:19:16.346008Z","iopub.status.idle":"2022-12-07T14:19:16.352644Z","shell.execute_reply.started":"2022-12-07T14:19:16.345971Z","shell.execute_reply":"2022-12-07T14:19:16.351372Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\n\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.15, 1.5)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T14:19:22.706009Z","iopub.execute_input":"2022-12-07T14:19:22.707079Z","iopub.status.idle":"2022-12-07T14:19:24.737125Z","shell.execute_reply.started":"2022-12-07T14:19:22.707037Z","shell.execute_reply":"2022-12-07T14:19:24.735855Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from functools import partial\n\ndef mk_quad(a, b, c): return partial(quad, a, b, c)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T14:20:05.123966Z","iopub.execute_input":"2022-12-07T14:20:05.124542Z","iopub.status.idle":"2022-12-07T14:20:05.131872Z","shell.execute_reply.started":"2022-12-07T14:20:05.124470Z","shell.execute_reply":"2022-12-07T14:20:05.130479Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def mae(pred, actual): return torch.abs(pred-actual).mean()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T14:20:18.118482Z","iopub.execute_input":"2022-12-07T14:20:18.119725Z","iopub.status.idle":"2022-12-07T14:20:18.124733Z","shell.execute_reply.started":"2022-12-07T14:20:18.119677Z","shell.execute_reply":"2022-12-07T14:20:18.123612Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def quad_mae(params):\n    f = mk_quad(*params)\n    return mae(f(x), y)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T14:20:23.511388Z","iopub.execute_input":"2022-12-07T14:20:23.511826Z","iopub.status.idle":"2022-12-07T14:20:23.517173Z","shell.execute_reply.started":"2022-12-07T14:20:23.511787Z","shell.execute_reply":"2022-12-07T14:20:23.516165Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch\nabc = torch.Tensor([1.1, 1.1, 1.1])","metadata":{"execution":{"iopub.status.busy":"2022-12-07T14:24:53.113185Z","iopub.execute_input":"2022-12-07T14:24:53.113633Z","iopub.status.idle":"2022-12-07T14:24:53.119328Z","shell.execute_reply.started":"2022-12-07T14:24:53.113585Z","shell.execute_reply":"2022-12-07T14:24:53.118126Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"abc.requires_grad_()\n\nloss = quad_mae(abc)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T14:24:55.332213Z","iopub.execute_input":"2022-12-07T14:24:55.332697Z","iopub.status.idle":"2022-12-07T14:24:55.339213Z","shell.execute_reply.started":"2022-12-07T14:24:55.332650Z","shell.execute_reply":"2022-12-07T14:24:55.337999Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad(): abc -= abc.grad*0.01\n    print(f'step={i}; loss={loss:.2f}')","metadata":{"execution":{"iopub.status.busy":"2022-12-07T14:20:40.957950Z","iopub.execute_input":"2022-12-07T14:20:40.959353Z","iopub.status.idle":"2022-12-07T14:20:40.990473Z","shell.execute_reply.started":"2022-12-07T14:20:40.959296Z","shell.execute_reply":"2022-12-07T14:20:40.989104Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"step=0; loss=2.42\nstep=1; loss=2.40\nstep=2; loss=2.36\nstep=3; loss=2.30\nstep=4; loss=2.21\nstep=5; loss=2.11\nstep=6; loss=1.98\nstep=7; loss=1.85\nstep=8; loss=1.72\nstep=9; loss=1.58\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Take Away\n\n1. [Matrix multiplication]() is the key to quickly calculate multuplication and addition of inputs and parameters.\n2. [Gradient descent]() is the tool used to understand how to minimize the loss function, since loss function composed by parameters ``abc``.\n3. [Rectified Linear function](), known as [ReLU](), is a linear function which takes input and the ouput is equals to the input. If the input is negative the output is zero. It's defined as: $f(x) = max(0, x)$","metadata":{}},{"cell_type":"markdown","source":"### Further Work\n1. Ankify:\n    - [ ] Matrix multiplication\n    - [ ] Gradient descent\n    - [ ] ReLU\n    - [ ] Foundamental inner loop to minimize the error which every DL model is based on\n2. [ ] Develop a Neural Network from scratch","metadata":{}},{"cell_type":"markdown","source":"---\n## References:\n- [Neural net foundations - Jeremy Howard, 2022](https://course.fast.ai/Lessons/lesson3.html)\n- [How does a neural net really work? - Jeremy Howard, 2022](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work)","metadata":{}}]}